############## Caret package ##############

# core functionality
# - preprocessing/cleaning data -> preProcess()
# - cross validation/data splitting->createDataPartition(), createResample(), createTimeSlices()
# - train algorithms on training data and apply to test sets -> train(), predict()
# - model comparison (evaluate the accuracy of model on new data) -> confusionMatrix()


### Example: Data splitting

# load packages and data
library(caret); library(kernlab); data(spam)
# create training set indexes with 75% of data
inTrain <- createDataPartition(y=spam$type,p=0.75, list=FALSE)
# subset spam data to training
training <- spam[inTrain,]
# subset spam data (the rest) to test
testing <- spam[-inTrain,]
# dimension of original and training dataset
rbind("original dataset" = dim(spam),"training set" = dim(training))


### Example: Fit a model

set.seed(32343)
modelFit <- train(type ~.,data=training, method="glm")
modelFit

#final model
modelFit$finalModel

#predict
predictions <- predict(modelFit,newdata=testing)
predictions

#table of cases of predicted and summary
confusionMatrix(predictions,testing$type)

# Caret tutorials:
#   http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
# http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf





############## Data splitting

# createDataPartition() - creates data partitions using given variable
# create training set indexes with 75% of data
inTrain <- createDataPartition(y=spam$type,p=0.75, list=FALSE)
# subset spam data to training
training <- spam[inTrain,]
# subset spam data (the rest) to test
testing <- spam[-inTrain,]
# dimension of original and training dataset
rbind("original dataset" = dim(spam),"training set" = dim(training))

#createFolds() - slices the data in to k folds for cross validation and returns k lists of indices
# create 10 folds for cross validation and return the training set indices
folds <- createFolds(y=spam$type,k=10,list=TRUE,returnTrain=TRUE)
# structure of the training set indices
str(folds)
# return the test set indices instead
# note: returnTrain = FALSE is unnecessary as it is the default behavior
folds.test <- createFolds(y=spam$type,k=10,list=TRUE,returnTrain=FALSE)
str(folds.test)
# return first 10 elements of the first training set
folds[[1]][1:10]

# createResample() - create 10 resamplings from the given data with replacement
# create 10 resamples
resamples <- createResample(y=spam$type,times=10,list=TRUE)
# structure of the resamples (note some samples are repeated)
str(resamples)

# createTimeSlices() - creates training sets with specified window length and the corresponding test sets
# create time series data
tme <- 1:1000
# create time slices
folds <- createTimeSlices(y=tme,initialWindow=20,horizon=10)
# name of lists
names(folds)
# first training set
folds$train[[1]]
# first test set
folds$test[[1]]



############## Training Options
# http://topepo.github.io/caret/training.html

# train() - function to apply the machine learning algorithm to construct model from training data
# returns the arguments of the default train function
args(train.default)

# trainControl() - creates an object that sets many options for 
#how the model will be applied to the training data
# returns the default arguments for the trainControl object
args(trainControl)


############## Plotting predictions
# http://caret.r-forge.r-project.org/visualizations.html
# it is important to only plot the data in the training set
# - using the test data may lead to over-fitting (model should not be adjusted to test set)
# - goal of producing these exploratory plots = look for potential outliers, skewness, imbalances in
# outcome/predictors, and explainable groups of points/patterns

# load relevant libraries
library(ISLR); library(ggplot2);
# load wage data
data(Wage)
# create training and test sets
inTrain <- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
# plot relationships between the predictors and outcome
featurePlot(x=training[,c("age","education","jobclass")], y = training$wage,plot="pairs")

# qplot plus linear regression lines
qplot(age,wage,colour=education,data=training)+geom_smooth(method='lm',formula=y~x)


# cut2(variable, g=3) = creates a new factor variable by cutting the specified variable into n groups
# (3 in this case) based on percentiles
# - Note: cut2 function is part of the Hmisc package, so library(Hmisc) must be run first
# - this variable can then be used to tabulate/plot the data
#  grid.arrange(p1, p2, ncol=2) = ggplot2 function the print multiple graphs on the same plot
# - Note: grid.arrange function is part of the gridExtra package, so library(gridExtra) must
# be run first

# load Hmisc and gridExtra packages
library(Hmisc);library(gridExtra);
# cute the wage variable
cutWage <- cut2(training$wage,g=3)
# view groups
table(cutWage)
# plot the boxplot
p1 <- qplot(cutWage,age, data=training,fill=cutWage,
            geom=c("boxplot"))
# plot boxplot and point clusters
p2 <- qplot(cutWage,age, data=training,fill=cutWage,
            geom=c("boxplot","jitter"))
# plot the two graphs side by side
grid.arrange(p1,p2,ncol=2)

# table(cutVariable, data$var2) = tabulates the cut factor variable vs another variable in the dataset
# (ie; builds a contingency table using cross-classifying factors)
#  prop.table(table, margin=1) = converts a table to a proportion table

# tabulate the cutWage and jobclass variables
t <- table(cutWage,training$jobclass)
# print table
t

# convert to proportion table based on the rows
prop.table(t,1)

# produce density plot
qplot(wage,colour=education,data=training,geom="density")

# Conclusion:
# Make your plots only in the training set
#   Don't use the test set for exploration!
# Things you should be looking for
#   Imbalance in outcomes/predictors
#   Outliers
#   Groups of points not explained by a predictor
#   Skewed variables

############## Preprocessing (tutorial)
#  some predictors may have strange distributions (i.e. skewed) and may need to be transformed to be
# more useful for prediction algorithm
# - particularly true for model based algorithms - naive Bayes, linear discriminate analysis, linear
# regression

# Example
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                               p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave. capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
#Result: sd is much more than mean


### Standardizing (centering)
#subtracting the observations of a particular variable by its mean
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(trainCapAveS)
sd(trainCapAveS)


### Scaling  
# dividing the observations of a particular variable by its standard deviation

### Normalizing
# centering and scaling the variable ! effectively converting each observation to the 
# number of standard deviations away from the mean
#   - the distribution of the normalized variable will have a mean of 0 and standard deviation of 1
#   - Note: normalizing data can help remove bias and high variability, but may not be applicable 
#           in all cases
# If a predictor/variable is standardized when training the model, the same transformations must
# be performed on the test set with the mean and standard deviation of the train variables
#   - this means that the mean and standard deviation of the normalized test variable will NOT be 0
# and 1, respectively, but will be close
#   - transformations must likely be imperfect but test/train sets must be processed the same way

# load spam data
data(spam)
# create train and test sets
inTrain <- createDataPartition(y=spam$type,p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
# create preProcess object for all predictors ("-58" because 58th = outcome)
preObj <- preProcess(training[,-58],method=c("center","scale"))
# normalize training set
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
# normalize test set using training parameters
testCapAveS <- predict(preObj,testing[,-58])$capitalAve
# compare results for capitalAve variable
rbind(train = c(mean = mean(trainCapAveS), std = sd(trainCapAveS)),
      test = c(mean(testCapAveS), sd(testCapAveS)))


### preprocess(data, method="BoxCox") = applies BoxCox transformations to continuous data to help
# normalize the variables through maximum likelihood

# set up BoxCox transforms
preObj <- preProcess(training[,-58],method=c("BoxCox"))
# perform preprocessing on training data
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
# plot histogram and QQ Plot
# Note: the transformation definitely helped to
# normalize the data but it does not produce perfect result
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)


### preProcess(data, method="knnImpute") = impute/estimate the missing data using k nearest
# neighbors (knn) imputation

# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA
# Impute and standardize
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve
# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
# compute differences between imputed values and true values
quantile(capAve - capAveTruth)




############## Covariate Creation/Feature Extraction
# [level 1]: construct covariate (usable metric, feature) from raw data depends heavily on application
# - ideally we want to summarize data without too much information loss
# - examples
# * text files: frequency of words, frequency of phrases (Google ngrams), frequency of capital
# letters
# * images: edges, corners, blobs, ridges (computer vision feature detection)
# * webpages: number and type of images, position of elements, colors, videos (A/B Testing)
# * people: height, weight, hair color, sex, country of origin
# - generally, more knowledge and understanding you have of the system/data, the easier it will be to
# extract the summarizing features

# [level 2]: construct new covariates from extracted covariate
# - generally transformations of features you extract from raw data
# - used more for methods like regression and support vector machines (SVM), whose accuracy depend
# more on the distribution of input variables
# - models like classification trees don’t require as many complex covariates
# - best approach is through exploratory analysis (tables/plots)
# - should only be performed on the train dataset
# - new covariates should be added to data frames under recognizable names so they can be used later


### Creating Dummy Variables
#convert factor variables to indicator/dummy variable -> qualitative become quantitative

# - dummyVars(outcome~var, data=training) = creates a dummy variable object that can be used
# through predict function to create dummy variables
# setting up data

# - predict(dummyObj, newdata=training) = creates appropriate columns to represent the factor
# variable with appropriate 0s and 1s
# * 2 factor variable -> two columns which have 0 or 1 depending on the outcome
# * 3 factor variable -> three columns which have 0, 0, and 1 representing the outcome
# * Note: only one of the columns can have values of 1 for each observation

inTrain <- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
# create a dummy variable object
dummies <- dummyVars(wage ~ jobclass,data=training)
# create the dummy variable columns
head(predict(dummies,newdata=training))


### Removing Zero Covariates
# some variables have no variability at all (i.e. variable indicating if an email contained letters)
# these variables are not useful when we want to construct a prediction model

# - nearZeroVar(training, saveMetrics=TRUE) = returns list of variables in training data set with
# information on frequency ratios, percent uniques, whether or not it has zero variance

# print nearZeroVar table
nearZeroVar(training,saveMetrics=TRUE)


### Creating Splines (Polynomial Functions)
# - when you want to fit curves through the data, basis functions can be leveraged
# - [splines package] bs(data$var, df=3) = creates 3 new columns corresponding to the var, var2, and
# var3 terms
# - ns() and poly() can also be used to generate polynomials
# - gam() function can also be used and it allows for smoothing of multiple variables with different values
# for each variable
# - Note: the same polynomial operations must be performed on the test sets using the predict function

# load splines package
library(splines)
# create polynomial function
bsBasis <- bs(training$age,df=3)
# fit the outcome on the three polynomial terms
lm1 <- lm(wage ~ bsBasis,data=training)
# plot all age vs wage data
plot(training$age,training$wage,pch=19,cex=0.5)
# plot the fitted polynomial function
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)

# predict on test values
head(predict(bsBasis,age=testing$age))


### Multicore Parallel Processing
# * many of the algorithms in the caret package are computationally intensive
# * since most of the modern machines have multiple cores on their CPUs, it is often wise to enable
# multicore parallel processing to expedite the computations
# * doMC package is recommended to be used for caret computations (reference)
# - doMC::registerDoMC(cores=4) = registers 4 cores for R to utilize
# - the number of cores you should specify depends on the CPU on your computer (system information
#                                                                               usually contains the number of cores)
# * it’s also possible to find the number of cores by directly searching for your CPU model number
# on the Internet
# - Note: once registered, you should see in your task manager/activity monitor that 4 “R Session”
# appear when you run your code







############## Correlated predictors ##############
# constructing a prediction model may not require every predictor
# ideally we want to capture the most variation with the least amount of variables
# - weighted combination of predictors may improve fit
# - combination needs to capture the most information

library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                               p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]

M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
#result: the list of the highly correlated variables

# fill vector by the values fromt the prevoius step
names(spam)[c(32,34,40)]

plot(spam[,34],spam[,32])
# as expected they are in one line

############## Preprocessing with Principal Component Analysis (PCA)
# PCA is suited to do this and will help reduce number of predictors as well as reduce noise (due to
#                                                                                             averaging)
# - statistical goal = find new set of multivariate variables that are uncorrelated and explain as much
# variance as possible
# - data compression goal = find the best matrix created with fewer variables that explains the original
# data
# - PCA is most useful for linear-type models (GLM, LDA)
# - generally more difficult to interpret the predictors (complex weighted sums of variables)
# - Note: outliers are can be detrimental to PCA as they may represent a lot of variation in data
# * exploratory analysis (plots/tables) should be used to identify problems with the predictors
# * transformations with log/BoxCox may be helpful


### #1 Make analysys by prcomp Function
# - pr<-prcomp(data) = performs PCA on all variables and returns a prcomp object that contains
# information about standard deviations and rotations

# - pr$rotations = returns eigenvectors for the linear combinations of all variables (coefficients
#                                                                                     that variables are multiplied by to come up with the principal components) ! how the principal
# components are created
# - often times, it is useful to take the log transformation of the variables and adding 1 before
# performing PCA
# * helps to reduce skewness or strange distribution in data
# * log(0) = - infinity, so we add 1 to account for zero values
# * makes data more Gaussian

# load spam data
data(spam)
# perform PCA on dataset
prComp <- prcomp(log10(spam[,-58]+1)) # make the data to be more gaussian
# print out the eigenvector/rotations first 5 rows and PCs
head(prComp$rotation[, 1:5], 5)

# create new variable that marks spam as 2 and nospam as 1
typeColor <- ((spam$type=="spam")*1 + 1)
# plot the first two principal components
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")

### #2 Make analysys by caret Package

# - pp<-preProcess(log10(training[,-58]+1),method="pca",pcaComp=2,thresh=0.8)) = perform
# PCA with preProcess function and returns the number of principal components that can capture the

# majority of the variation
# - creates a preProcess object that can be applied using predict function
# - pcaComp=2 = specifies the number of principal components to compute (2 in this case)
# - thresh=0.8 = threshold for variation captured by principal components
# * thresh=0.95 = default value, which returns the number of principal components that are
# needed to capture 95% of the variation in data

# - predict(pp, training) = computes new variables for the PCs (2 in this case) for the training data
# set

# create train and test sets
inTrain <- createDataPartition(y=spam$type,p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
# create preprocess object
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
# calculate PCs for training data
trainPC <- predict(preProc,log10(training[,-58]+1))
# run model on outcome and principle components
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
# calculate PCs for test data
testPC <- predict(preProc,log10(testing[,-58]+1))
# compare results
confusionMatrix(testing$type,predict(modelFit,testPC))
# result: accuracy is >90%


# alternatively, PCA can be directly performed with the train method
# - train(outcome ~ ., method="glm", preProcess="pca", data=training) = performs PCA
# first on the training set and then runs the specified model
# * effectively the same procedures as above (preProcess -> predict)

# construct model
modelFit <- train(training$type ~ .,method="glm",preProcess="pca",data=training)
# print results of model
confusionMatrix(testing$type,predict(modelFit,testing))

### Final thoughts on PCs

# Most useful for linear-type models
# Can make it harder to interpret predictors
# Watch out for outliers!
#   Transform first (with logs/Box Cox)
#   Plot predictors to identify problems
# For more info see
#   Exploratory Data Analysis
#   Elements of Statistical Learning (http://statweb.stanford.edu/~tibs/ElemStatLearn/)


############## Predicting with Regression ############## 
# prediction with regression = fitting regression model (line) to data ! multiplying each variable by
# coefficients to predict outcome
# - useful when the relationship between the variables can be modeled as linear
# - the model is easy to implement and the coefficients are easy to interpret
# - if the relationships are non-linear, the regression model may produce poor results/accuracy

# load data
data(faithful)
# create train and test sets
inTrain <- createDataPartition(y=faithful$waiting, p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
# build linear model
lm1 <- lm(eruptions ~ waiting,data=trainFaith)
# print summary of linear model
summary(lm1)

# predict eruptions for new waiting time
newdata <- data.frame(waiting=80)
predict(lm1,newdata)

# create 1 x 2 panel plot
par(mfrow=c(1,2))
# plot train data with the regression line
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",
     ylab="Duration", main = "Train")
lines(trainFaith$waiting,predict(lm1),lwd=3)
# plot test data with the regression line
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",
     ylab="Duration", main = "Test")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)

# Calculate RMSE on training and test sets
c(trainRMSE = sqrt(sum((lm1$fitted-trainFaith$eruptions)^2)),
  testRMSE = sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2)))
#result: the error is almost the same. the test set error is always larger anyway.


### Prediction intervals
# pi<-predict(lm, newdata=test, interval="prediction") = returns 3 columns for fit (predicted
#                                                                                   value, same as before), lwr (lower bound of prediction interval), and upr (upper bound of prediction
#                                                                                                                                                              interval)
# - matlines(x, pi, type="l") = plots three lines, one for the linear fit and two for upper/lower
# prediction interval bounds

# #1 calculate prediction interval
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
# plot data points (eruptions, waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
# plot fit line and prediction interval
matlines(testFaith$waiting,pred1,type="l",,col=c(1,2,2),lty = c(1,1,1), lwd=3)
# result: all possible predictions should be between red lines

# #2 calculate with caret
# create train and test sets
inTrain <- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
# fit linear model for age jobclass and education
modFit<- train(wage ~ age + jobclass + education,method = "lm",data=training)
# store final model
finMod <- modFit$finalModel
# set up 2 x 2 panel plot
par(mfrow = c(2, 2))
# construct diagnostic plots for model
plot(finMod,pch=19,cex=0.5,col="#00000010")

### Notes and further reading
# Regression models with multiple covariates can be included
# Often useful in combination with other models
# Elements of statistical learning (http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
# Modern applied statistics with S (http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570)
# Introduction to statistical learning (http://www-bcf.usc.edu/~gareth/ISL/)


############## Predicting with Regression Multi Covariates Example
library(ISLR); library(ggplot2); library(caret);
data(Wage); Wage <- subset(Wage,select=-c(logwage))
summary(Wage)

# Get training/test sets
inTrain <- createDataPartition(y=Wage$wage,
                               p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
dim(training); dim(testing)

# Feature plot
featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")
# result: hard to see, but can see clusters (bottom right, top left) and outliers AND
#the distributions

#Plot age versus wage
qplot(age,wage,data=training)
# result: can see outliers at the top 

# Plot age versus wage colour by jobclass
qplot(age,wage,colour=jobclass,data=training)
# result: can see that the data by jobclass is mixed, but most outliers at the top are 
#by Information joblass (has higher variability)

# Plot age versus wage colour by education
qplot(age,wage,colour=education,data=training)
# result: advanced degree explains the variation at the top

# Result: The relation between jobclass and enducation explains the higher wage 


### Fit linear model
#run linear model on the training data
# We assume that the wage is dependant on age, jobclass, education

# create train and test sets
inTrain <- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
# fit linear model for age jobclass and education
modFit<- train(wage ~ age + jobclass + education,method = "lm",data=training)
# store final model
finMod <- modFit$finalModel
# set up 2 x 2 panel plot
par(mfrow = c(2, 2))

###plotting residuals by fitted values and coloring with a variable not used in the model helps spot a trend
#in that variable
# construct diagnostic plots for model
plot(finMod,pch=19,cex=0.5,col="#00000010")
par(mfrow = c(1, 1))
# result: Residuals vs Fitted: line is near zero, some outliers above

###plotting residuals by index (ie; row numbers) can be helpful in showing missing variables
# plot fitted values by residuals
qplot(finMod$fitted, finMod$residuals, color=race, data=training)
# result: the top of the outliers belongs to black race

###plot residual by index
plot(finMod$residuals,pch=19,cex=0.5)
#here the residuals increase linearly with the index, and the highest residuals are concentrated in the
#higher indexes, so there must be a missing variable

# Predicted versus truth in test set
pred <- predict(modFit, testing)
qplot(wage,pred,colour=year,data=testing)
# statistics by year

# If you want to use all covariates
modFitAll<- train(wage ~ .,data=training,method="lm")
pred <- predict(modFitAll, testing)
qplot(wage,pred,data=testing)

# Notes and further reading
#   Often useful in combination with other models
#   Elements of statistical learning (http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
#   Modern applied statistics with S (http://www.amazon.com/Modern-Applied-Statistics-W-N-Venables/dp/0387954570)
#   Introduction to statistical learning (http://www-bcf.usc.edu/~gareth/ISL/)


